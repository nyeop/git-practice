{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 10\n",
    "\n",
    "In this week's lab class you will gain experience with applying some Bayesian techniques in practice. There will be a quiz based on the worksheet, which you should complete first. The quiz is worth 5% of the assessment of the unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian regression\n",
    "\n",
    "We are going to fit a polynomial to a one-dimensional problem and explore its properties. First we import some useful packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import numpy.linalg as la\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define a utility function that will create a non-linear function `f` of a vector `x` with noise added with standard deviation `noise_amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, noise_amount):\n",
    "    y = np.sqrt(x) * np.sin(x)\n",
    "    noise = np.random.normal(0, 1, len(x))\n",
    "    return y + noise_amount * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function to plot results that we will use a lot in the following experiments. The four arguments it takes are a (fitted) Bayesian ridge regression model and a (fitted) ordinary regression model, the degree of the model and the number of training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(clf, ols, degree, num_training_points):\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(\"Weights of the model with n=\"+ str(num_training_points))\n",
    "    plt.plot(clf.coef_, color='lightgreen', linewidth=lw,\n",
    "             label=\"Bayesian Ridge estimate\")\n",
    "    #plt.plot(w, color='gold', linewidth=lw, label=\"Ground truth\")\n",
    "    plt.plot(ols.coef_, color='navy', linestyle='--', label=\"Linear regression estimate\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Values of the weights\")\n",
    "    plt.legend(loc=\"best\", prop=dict(size=12))\n",
    "    \n",
    "    X_plot = np.linspace(0, 11, 30)\n",
    "    y_plot = f(X_plot, noise_amount=0)\n",
    "    y_mean, y_std = clf.predict(np.vander(X_plot, degree), return_std=True)\n",
    "    y2_mean = ols.predict(np.vander(X_plot, degree))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.errorbar(X_plot, y_mean, y_std, color='navy',\n",
    "                 label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)\n",
    "    plt.plot(X_plot, y2_mean,  color='red', label='Linear Regression',\n",
    "             linestyle='--', linewidth=lw)\n",
    "    plt.plot(X_plot, y_plot, color='gold', linewidth=lw,\n",
    "             label=\"Ground Truth\")\n",
    "    plt.ylabel(\"Output y\")\n",
    "    plt.xlabel(\"Feature X\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the data generating function looks like, set the random seed, generate 100 equally spaced points (use np.linspace) in the range $(1, 10)$, and plot `f(x)` against `x` with `noise_amount=0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to fit polynomials using linear regression to various datasets generated with this function. To create the input functions, we will use the numpy function vander: API available [here](https://numpy.org/doc/stable/reference/generated/numpy.vander.html) \n",
    "\n",
    "This function generates polynomial basis function values from a row vector. Use it to create a matrix up to third order for the vector $(1, 2, 3)$ and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is worth reading the sklearn documentation about Bayesian ridge regression which can be found [here](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression) (and the earlier part of Section 1.1.10). The parameter $\\alpha$ is the precision (inverse variance) of the noise model (what we called $\\beta$ in the lectures) while the parameter $\\lambda$ is the precision of the weight prior (what we called $\\alpha$ in the lectures). \n",
    "\n",
    "You should create the Bayesian ridge regression model with `compute_score=True` and fit it with degree equal to 10 to 100 equally spaced points in the range $(1, 10)$ (remember to set the seed to 1729 first) and noise standard deviation $0.1$. The input values should be generated using the vander function at the data points.\n",
    "\n",
    "Do the same for a linear regression model and then call `plot_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first ![First graph](graph1.png) and second graphs ![Second graph](graph2.png)\n",
    "\n",
    "If you have got that right, you are ready to carry out your first experiment. Let the number of points in the training set\n",
    "vary in the selection $(8, 9, 10, 11, 12, 20, 40, 100)$ and repeat the fitting process above for each of these training\n",
    "sets (reset the random seed on each iteration). Print out the final fitted value of `alpha` in the Bayesian ridge \n",
    "regression model at each iteration of this process. Compare it to what the true value is.\n",
    "\n",
    "Look at all the graphs you generate and the values of weights and parameters. Write down your analysis of these results - you will need this for the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the experiment above but with the `noise_amount` parameter set to 1.0 instead of 0.1. Again, note the values of\n",
    "`alpha` and analyse the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence procedure for PCA\n",
    "In this section we are going to do some experiments to compare the evidence procedure for PCA with cross-validation and also with a related model called Factor Analysis. First we import some relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create some synthetic data. There are 1000 data points in 50-dimensional space and the data has rank 10. We achieve this by using singular value decomposition to create a lower rank basis. The homogeneous noise data `X_hom` has the same variance in each dimension while the heterogeneous noise data `X_het` has randomly varying variance in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "n_samples, n_features, rank = 1000, 50, 10\n",
    "sigma = 1.\n",
    "rng = np.random.RandomState(42)\n",
    "U, _, _ = la.svd(rng.randn(n_features, n_features))\n",
    "X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n",
    "\n",
    "# Adding homoscedastic noise\n",
    "X_hom = X + sigma * rng.randn(n_samples, n_features)\n",
    "\n",
    "# Adding heteroscedastic noise\n",
    "sigmas = sigma * rng.rand(n_features) + sigma / 2.\n",
    "X_het = X + rng.randn(n_samples, n_features) * sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `compute_scores` takes a dimensionality reduction model and uses cross-validation to \n",
    "compute the log likelihood of it when\n",
    "fitted to a dataset a range of numbers of components using the `sklearn` function `cross_val_score`: information\n",
    "    [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). It does this\n",
    "    by fitting the model and applying 5-fold cross-validation (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(0, n_features, 1)  # options for n_components\n",
    "\n",
    "def compute_scores(X, model):\n",
    "    \n",
    "    scores = []\n",
    "    for n in n_components:\n",
    "        model.n_components = n\n",
    "        scores.append(np.mean(cross_val_score(model, X)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to compare PCA with the evidence procedure with PCA using cross-validation and factor analysis using cross-validation. Factor analysis is very similar to probabilistic PCA but the noise model is diagonal instead of spherical (also\n",
    "known as *isotropic*): see [API](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html).\n",
    "\n",
    "You should create a PCA model with the full SVD solver and a Factor Analysis model. Fit and score them (using `compute_scores`) for all numbers of components from 1 to 50 and store the result in a variable `pca_scores` and `fa_scores` respectively. Measure how long `compute_scores` takes for each model and print the results.\n",
    "\n",
    "Then fit a single PCA using mle (which ensures that the evidence procedure is used). Time this as well and print the result.\n",
    "\n",
    "Do this for the homogeneous data `X_hom`. The code may take 1-2 minutes to run. Review the graph and timings and analyse what they tell you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = X_hom\n",
    "title = 'Homoscedastic Noise'\n",
    "\n",
    "#TODO block 6a \n",
    "#Do cross-validation here\n",
    "\n",
    "\n",
    "n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "#TODO block 6b\n",
    "# Do evidence procedure here\n",
    "\n",
    "n_components_pca_mle = pca.n_components_\n",
    "\n",
    "print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n",
    "plt.plot(n_components, fa_scores, 'r', label='FA scores')\n",
    "plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n",
    "plt.axvline(n_components_pca, color='b',\n",
    "            label='PCA CV: %d' % n_components_pca, linestyle='--')\n",
    "plt.axvline(n_components_fa, color='r',\n",
    "            label='FactorAnalysis CV: %d' % n_components_fa,\n",
    "            linestyle='--')\n",
    "plt.axvline(n_components_pca_mle, color='k',\n",
    "            label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n",
    "\n",
    "\n",
    "plt.xlabel('nb of components')\n",
    "plt.ylabel('CV scores')\n",
    "plt.legend(loc='best')\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now carry out the same experiment for the heterogeneous dataset `X_het` and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = X_het\n",
    "title = 'Heteroscedastic Noise'\n",
    "\n",
    "#TODO block 7a\n",
    "\n",
    "\n",
    "\n",
    "n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "#TODO block 7b\n",
    "\n",
    "n_components_pca_mle = pca.n_components_\n",
    "\n",
    "print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n",
    "plt.plot(n_components, fa_scores, 'r', label='FA scores')\n",
    "plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n",
    "plt.axvline(n_components_pca, color='b',\n",
    "            label='PCA CV: %d' % n_components_pca, linestyle='--')\n",
    "plt.axvline(n_components_fa, color='r',\n",
    "            label='FactorAnalysis CV: %d' % n_components_fa,\n",
    "            linestyle='--')\n",
    "plt.axvline(n_components_pca_mle, color='k',\n",
    "            label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n",
    "\n",
    "\n",
    "plt.xlabel('nb of components')\n",
    "plt.ylabel('CV scores')\n",
    "plt.legend(loc='best')\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Gaussian mixture models\n",
    "In this section we will see how variational Bayesian methods help us to fit Gaussian mixture models. First we load and normalise the Old Faithful dataset. An overview of the sklearn mixture models can be found [here](https://scikit-learn.org/stable/modules/mixture.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "\n",
    "df = pd.read_excel('OldFaithfulData.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a data matrix and then standardise the data (normalise each variable to zero mean and unit variance). This is often a good pre-processing step to apply if you are fitting non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = df.to_numpy()\n",
    "\n",
    "scaler = StandardScaler().fit(data)\n",
    "X = scaler.transform(data)\n",
    "\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code, we create a function that we will use to plot the results of fitting various mixture models. In this function, the first parameter is the data, the second is the predicted label at the data, the third and fourth are the means and covariances extracted from the mixture model. The index should be incremented by one each time it is called since it gives the index of the sub-plot on that iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',\n",
    "                              'darkorange', 'green'])\n",
    "\n",
    "def plot_gmm_results(X, Y_, means, covariances, index, title):\n",
    "    plt.figure(figsize=(10,8));\n",
    "    \n",
    "    splot, ax = plt.subplots(subplot_kw={'aspect': 'equal'})\n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "            means, covariances, color_iter)):\n",
    "        v, w = la.eigh(covar)\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / la.norm(w[0])\n",
    "        # as the DP will not use every component it has access to\n",
    "        # unless it needs it, we shouldn't plot the redundant\n",
    "        # components.\n",
    "        if not np.any(Y_ == i):\n",
    "            continue\n",
    "        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "        # Plot an ellipse to show the Gaussian component\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "\n",
    "    plt.xlim(-2., 2.)\n",
    "    plt.ylim(-2.5, 2.)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a Gaussian mixture model (`mixture.GaussianMixture`) with 6 components, full covariance matrices, a random state of 1729 and fit it to the dataset `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a Bayesian GMM (`mixture.BayesianGaussianMixture`) with the same parameters, but also specifying the `weight_concentration_prior_type` to be `'dirichlet_distribution'`. Plot the results (with `index=1`). This is the \n",
    "[API](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of the mixing coefficient hyperprior (known as the *weight concentration prior* in this implementation) is \n",
    "`1/n_components`. Now carry out the same experiment setting it to $10^{-3}$, $1$, and $1000$. Then review the five graphs (two above and three below) and write down your analysis of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO block 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
